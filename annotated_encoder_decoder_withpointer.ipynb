{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "seq_to_seq_sum.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "N55sJZGWxCyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DRIVE_FOLDER = \"My Drive/final model\" # Optional path to find the rest of the python files if it's running inside google colab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nQsSuIhT6BzM",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19B3aGenw79y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "520f65f2-23fd-42c5-985f-ca242cb3a584"
      },
      "source": [
        "if IN_COLAB:\n",
        "  from google.colab import drive\n",
        "  import os\n",
        "\n",
        "  drive_mount_path = '/content/drive'\n",
        "  \n",
        "  drive.mount(drive_mount_path)\n",
        "  os.chdir(os.path.join(drive_mount_path, DRIVE_FOLDER))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yBAtW1Ik7Ea1",
        "outputId": "07eeb303-7608-4a6b-d3c6-7926ac4a4950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "!python data/make_google_data.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Downloading https://github.com/google-research-datasets/sentence-compression/raw/master/data/sent-comp.train01.json.gz...\n",
            "Processing sent-comp.train01.json.gz...\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "Downloading https://github.com/google-research-datasets/sentence-compression/raw/master/data/sent-comp.train02.json.gz...\n",
            "Processing sent-comp.train02.json.gz...\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "Downloading https://github.com/google-research-datasets/sentence-compression/raw/master/data/sent-comp.train03.json.gz...\n",
            "Processing sent-comp.train03.json.gz...\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "Downloading https://github.com/google-research-datasets/sentence-compression/raw/master/data/sent-comp.train04.json.gz...\n",
            "Processing sent-comp.train04.json.gz...\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n",
            "68000\n",
            "69000\n",
            "70000\n",
            "71000\n",
            "72000\n",
            "73000\n",
            "74000\n",
            "75000\n",
            "76000\n",
            "77000\n",
            "78000\n",
            "79000\n",
            "80000\n",
            "Downloading https://github.com/google-research-datasets/sentence-compression/raw/master/data/sent-comp.train05.json.gz...\n",
            "Processing sent-comp.train05.json.gz...\n",
            "81000\n",
            "82000\n",
            "83000\n",
            "84000\n",
            "85000\n",
            "86000\n",
            "87000\n",
            "88000\n",
            "89000\n",
            "90000\n",
            "91000\n",
            "92000\n",
            "93000\n",
            "94000\n",
            "95000\n",
            "96000\n",
            "97000\n",
            "98000\n",
            "99000\n",
            "100000\n",
            "Downloading https://github.com/google-research-datasets/sentence-compression/raw/master/data/sent-comp.train06.json.gz...\n",
            "Processing sent-comp.train06.json.gz...\n",
            "101000\n",
            "102000\n",
            "103000\n",
            "104000\n",
            "105000\n",
            "106000\n",
            "107000\n",
            "108000\n",
            "109000\n",
            "110000\n",
            "111000\n",
            "112000\n",
            "113000\n",
            "114000\n",
            "115000\n",
            "116000\n",
            "117000\n",
            "118000\n",
            "119000\n",
            "120000\n",
            "Downloading https://github.com/google-research-datasets/sentence-compression/raw/master/data/sent-comp.train07.json.gz...\n",
            "Processing sent-comp.train07.json.gz...\n",
            "121000\n",
            "122000\n",
            "123000\n",
            "124000\n",
            "125000\n",
            "126000\n",
            "127000\n",
            "128000\n",
            "129000\n",
            "130000\n",
            "131000\n",
            "132000\n",
            "133000\n",
            "134000\n",
            "135000\n",
            "136000\n",
            "137000\n",
            "138000\n",
            "139000\n",
            "140000\n",
            "Downloading https://github.com/google-research-datasets/sentence-compression/raw/master/data/sent-comp.train08.json.gz...\n",
            "Processing sent-comp.train08.json.gz...\n",
            "141000\n",
            "142000\n",
            "143000\n",
            "144000\n",
            "145000\n",
            "146000\n",
            "147000\n",
            "148000\n",
            "149000\n",
            "150000\n",
            "151000\n",
            "152000\n",
            "153000\n",
            "154000\n",
            "155000\n",
            "156000\n",
            "157000\n",
            "158000\n",
            "159000\n",
            "160000\n",
            "Downloading https://github.com/google-research-datasets/sentence-compression/raw/master/data/sent-comp.train09.json.gz...\n",
            "Processing sent-comp.train09.json.gz...\n",
            "161000\n",
            "162000\n",
            "163000\n",
            "164000\n",
            "165000\n",
            "166000\n",
            "167000\n",
            "168000\n",
            "169000\n",
            "170000\n",
            "171000\n",
            "172000\n",
            "173000\n",
            "174000\n",
            "175000\n",
            "176000\n",
            "177000\n",
            "178000\n",
            "179000\n",
            "180000\n",
            "Downloading https://github.com/google-research-datasets/sentence-compression/raw/master/data/sent-comp.train10.json.gz...\n",
            "Processing sent-comp.train10.json.gz...\n",
            "181000\n",
            "182000\n",
            "183000\n",
            "184000\n",
            "185000\n",
            "186000\n",
            "187000\n",
            "188000\n",
            "189000\n",
            "190000\n",
            "191000\n",
            "192000\n",
            "193000\n",
            "194000\n",
            "195000\n",
            "196000\n",
            "197000\n",
            "198000\n",
            "199000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-78AIVg-w7O9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fmdP3KveA1Ko",
        "colab": {}
      },
      "source": [
        "import random\n",
        "fin = open(\"data/sentences.txt\", 'rb')\n",
        "f75out = open(\"data/train.txt\", 'wb')\n",
        "f25out = open(\"data/test.txt\", 'wb')\n",
        "for line in fin:\n",
        "    r = random.random()\n",
        "    if r < 0.75:\n",
        "        f75out.write(line)\n",
        "    else:\n",
        "        f25out.write(line)\n",
        "fin.close()\n",
        "f75out.close()\n",
        "f25out.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wOzGXlvkDZbK",
        "outputId": "4495850c-7e1a-40cc-841e-4119d6a0ca49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        }
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading dataset /content/drive/My Drive/final model/data/train.txt... 150236 pairs.\n",
            "Building vocabulary... 30004 words.\n",
            "Training 3609207 trainable parameters...\n",
            "Epoch 1: 100% 1000/1000 [05:15<00:00,  3.65it/s, loss=2.87445]\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Seq2Seq. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type EncoderRNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DecoderRNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Bilinear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "Epoch 2: 100% 1000/1000 [05:13<00:00,  2.92it/s, loss=2.44922]\n",
            "Epoch 3: 100% 1000/1000 [05:17<00:00,  2.63it/s, loss=2.44242]\n",
            "Epoch 4: 100% 1000/1000 [05:17<00:00,  3.70it/s, loss=2.42868]\n",
            "Epoch 5: 100% 1000/1000 [05:13<00:00,  3.14it/s, loss=2.39895]\n",
            "Epoch 6:  50% 502/1000 [02:38<02:33,  3.25it/s, loss=2.39132]"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}