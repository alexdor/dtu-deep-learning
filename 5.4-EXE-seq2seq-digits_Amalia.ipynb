{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xDxWv1AgD0BF"
   },
   "source": [
    "# Seq2Seq - Encoder/Decoder networks\n",
    "In this exercise we'll have a deeper look into the ability to use multiple RNN's to infer and generate sequences of data.\n",
    "Specifically we will implement a Encoder-Decoder RNN based for a simple sequence to sequence translation task.\n",
    "This type of models have shown impressive performance in Neural Machine Translation and Image Caption generation. \n",
    "\n",
    "In the encoder-decoder structure one RNN (blue) encodes the input into a hidden representation, and a second RNN (red) uses this representation to predict the target values.\n",
    "An essential step is deciding how the encoder and decoder should communicate.\n",
    "In the simplest approach you use the last hidden state of the encoder to initialize the decoder.\n",
    "This is what we will do in this notebook, as shown here:\n",
    "\n",
    "![](./images/enc-dec.png)\n",
    "\n",
    "In this exercise we will translate from the words of number (e.g. 'nine') to the actual number (e.g. '9').\n",
    "The input for the Encoder RNN consists of words defining the number, whilst the output of such an encoding serves as input for the Decoder RNN that aims to generate generate a number. \n",
    "Our dataset is generated and consists of numbers and an End-of-Sentence (EOS) character ('#'). The data we want to generate should be like follows:\n",
    "\n",
    "```\n",
    "Examples: \n",
    "prediction  |  input\n",
    "991136#00 \t nine nine one one three six\n",
    "81771#000 \t eight one seven seven one\n",
    "3519614#0 \t three five one nine six one four\n",
    "26656#000 \t two six six five six\n",
    "60344#000 \t six zero three four four\n",
    "162885#00 \t one six two eight eight five\n",
    "78612625# \t seven eight six one two six two five\n",
    "9464710#0 \t nine four six four seven one zero\n",
    "191306#00 \t one nine one three zero six\n",
    "10160378# \t one zero one zero six three seven eight\n",
    "```\n",
    "\n",
    "Let us define the space of characters and numbers to be learned with the networks:\n",
    "\n",
    "```\n",
    "Number of valid characters: 27\n",
    "'0'=0,\t'1'=1,\t'2'=2,\t'3'=3,\t'4'=4,\t'5'=5,\t'6'=6,\t'7'=7,\t'8'=8,\t'9'=9,\t'#'=10,\t' '=11,\t'e'=12,\t'g'=13,\t'f'=14,\t'i'=15,\t'h'=16,\t'o'=17,\t'n'=18,\t's'=19,\t'r'=20,\t'u'=21,\t't'=22,\t'w'=23,\t'v'=24,\t'x'=25,\t'z'=26,\t\n",
    "Stop/start character = #\n",
    "```\n",
    "\n",
    "All represented characters and numbers as characters, gets mapped to an integer from 0-26. Our total space of valid characters consists of 27."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1692,
     "status": "ok",
     "timestamp": 1575204550704,
     "user": {
      "displayName": "Alexandros Dorodoulis",
      "photoUrl": "",
      "userId": "07816985308410186091"
     },
     "user_tz": -60
    },
    "id": "eOcni0JEfv1j",
    "outputId": "d3173ca8-9b17-4620-98a5-9e9c10ad6df2"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fedff8a4a445>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/Notes/Dtu courses/3rd_semester/Deep learning/dtu-deep-learning'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "try\n",
    "    from google.colab import drive\n",
    "    import os\n",
    "    drive.mount('/content/gdrive')\n",
    "    os.chdir('/content/gdrive/My Drive/Notes/Dtu courses/3rd_semester/Deep learning/dtu-deep-learning')\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4578,
     "status": "ok",
     "timestamp": 1575204553604,
     "user": {
      "displayName": "Alexandros Dorodoulis",
      "photoUrl": "",
      "userId": "07816985308410186091"
     },
     "user_tz": -60
    },
    "id": "_e5UcD5lD0BJ",
    "outputId": "3935d856-a123-424a-c358-e8c1874dd155"
   },
   "outputs": [],
   "source": [
    "from data_generator import generate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "device =  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device in use:\", device)\n",
    "\n",
    "NUM_INPUTS = 27 #No. of possible characters\n",
    "NUM_OUTPUTS = 11  # (0-9 + '#')\n",
    "\n",
    "### Hyperparameters and general configs\n",
    "MAX_SEQ_LEN = 8\n",
    "MIN_SEQ_LEN = 5\n",
    "TRAINING_SIZE = 100\n",
    "LEARNING_RATE = 0.003\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# Hidden size of enc and dec need to be equal if last hidden of encoder becomes init hidden of decoder\n",
    "# Otherwise we would need e.g. a linear layer to map to a space with the correct dimension\n",
    "NUM_UNITS_ENC = NUM_UNITS_DEC = 256\n",
    "HIDDEN_DIM = 512\n",
    "TEST_SIZE = 100\n",
    "EPOCHS = 10\n",
    "TEACHER_FORCING = 0.5\n",
    "NUM_OF_BATCHES=8\n",
    "\n",
    "# assert TRAINING_SIZE % NUM_OF_BATCHES == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gKWbvEj4D0BW"
   },
   "source": [
    "For this exercise we won´t worry about data generation, but utilise a built function for this purpose. The function generates random data constained by the 27 characters described above.\n",
    "\n",
    "The encoder takes as input the embedded text strings generated from the *generate* function as given here above ie. 'nine' would become [18 15 18 12].\n",
    "Sequeneces are generated at random given settings of minima and maxima length, constrained by the dimensions of the two RNN´s architecture.\n",
    "We may visualise a subset of the data generated by running the command below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# import os\n",
    "# drive.mount('/content/gdrive')\n",
    "# os.chdir('/content/gdrive/My Drive/Notes/Dtu courses/3rd_semester/Deep learning/dtu-deep-learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7152,
     "status": "ok",
     "timestamp": 1575204556189,
     "user": {
      "displayName": "Alexandros Dorodoulis",
      "photoUrl": "",
      "userId": "07816985308410186091"
     },
     "user_tz": -60
    },
    "id": "uvINzS2eD0BX",
    "outputId": "2bae650c-9fac-4faf-e9f7-9c28b1ea1b64"
   },
   "outputs": [],
   "source": [
    " !python data_generator.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TnhOaNtHD0Be"
   },
   "source": [
    "## Let's define the two RNN's\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kNWiuJAvD0Bg"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, self.emb_size)\n",
    "        self.rnn = nn.GRU(\n",
    "            self.emb_size,\n",
    "            self.hidden_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs, hidden, inputs_len):\n",
    "        # Input shape [batch, seq_in_len]\n",
    "        # inputs = [inputs[0],inputs[2]]\n",
    "        inputs = inputs.long()\n",
    "\n",
    "        # Embedded shape [batch, seq_in_len, embed]\n",
    "        embedded = self.dropout(self.embedding(inputs))\n",
    "        # embedded = embedded.view(embedded.shape[0]*embedded.shape[1],embedded.shape[2],embedded.shape[3])\n",
    "\n",
    "        # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        packed_embedded = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, inputs_len, batch_first=True\n",
    "        )\n",
    "\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "\n",
    "        # Output shape [batch, seq_in_len, embed]\n",
    "        # Hidden shape [1, batch, embed], last hidden state of the GRU cell\n",
    "        # We will feed this last hidden state into the decoder\n",
    "        # print(embedded.shape)\n",
    "        # Reshape our output to match the input shape of our forward pass\n",
    "        # hidden = hidden.reshape(hidden.shape[0],1, hidden.shape[1],hidden.shape[2])\n",
    "        # hidden=hidden.unsqueeze_(0)\n",
    "        # print(hidden[1].shape)\n",
    "        # view(len(sentence), 1, -1)\n",
    "        # print(test.shape)\n",
    "        # outputs,hidden = self.rnn(embedded)\n",
    "        # print(hidden.shape)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_outputs, batch_first=True\n",
    "        )\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2], hidden[-1]), dim=1)))\n",
    "        return outputs, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        init = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "        return init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9bkqDKVfRHw"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.Linear((hidden_size * 2) + hidden_size, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "\n",
    "        # hidden = [batch size, dec hid dim]\n",
    "        # encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        # mask = [batch size, src sent len]\n",
    "\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "\n",
    "        # encoder output =  [33, 8, 512], hidden = [8, 256]\n",
    "        # print(encoder_outputs.shape[0], encoder_outputs.shape[1])\n",
    "\n",
    "        # repeat encoder hidden state src_len times\n",
    "        # print(hidden.shape,hidden.unsqueeze(1).shape)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "\n",
    "        # encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # print(encoder_outputs.shape)\n",
    "\n",
    "        # hidden = [batch size, src sent len, dec hid dim]\n",
    "        # encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "        # print(hidden.shape,encoder_outputs.shape)\n",
    "\n",
    "        # encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        energy = torch.tanh(\n",
    "            self.attn(torch.cat((hidden, encoder_outputs), dim=2))\n",
    "        )\n",
    "\n",
    "        # energy = [batch size, src sent len, dec hid dim]\n",
    "\n",
    "        energy = energy.permute(0, 2, 1)\n",
    "\n",
    "        # energy = [batch size, dec hid dim, src sent len]\n",
    "\n",
    "        # v = [dec hid dim]\n",
    "\n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
    "\n",
    "        # v = [batch size, 1, dec hid dim]\n",
    "\n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "\n",
    "        # print(attention.shape,mask.shape)\n",
    "\n",
    "        # attention = [batch size, src sent len]\n",
    "\n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZtQnQrtHD0Bm"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, emb_size, output_size, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.emb_size = emb_size\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.emb_size)\n",
    "        self.out = nn.Linear(\n",
    "            (self.hidden_size * 2) + self.hidden_size + self.emb_size,\n",
    "            output_size,\n",
    "        )\n",
    "        self.rnn = nn.GRU(\n",
    "            (self.hidden_size * 2) + self.emb_size, self.hidden_size,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs, hidden, encoder_outputs, mask):\n",
    "        # Input shape: [batch, output_len]\n",
    "        # Hidden shape: [seq_len=1, batch_size, hidden_dim] (the last hidden state of the encoder)\n",
    "        dec_input = inputs.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(dec_input))\n",
    "        # print(embedded.shape,dec_input.shape)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        # print(hidden.shape,encoder_outputs.shape,mask.shape)\n",
    "        # encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "        a = a.unsqueeze(1)\n",
    "        # encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # print(a.shape, encoder_outputs.shape)\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        # print(embedded.shape,weighted.shape)\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        # print(weighted.shape,embedded.shape,rnn_input.shape,hidden.shape)\n",
    "        out, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        # out, hidden = self.rnn(self.embedding(dec_input), hidden)\n",
    "        assert (out == hidden).all()\n",
    "        embedded = embedded.squeeze(0)\n",
    "        out = out.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "\n",
    "        output = self.out(torch.cat((out, weighted, embedded), dim=1))\n",
    "        # print(output.shape,hidden.squeeze(0).shape)\n",
    "\n",
    "        # output = torch.stack(output).permute(1, 0, 2)  # [batch_size x seq_len x output_size]\n",
    "        hidden = hidden.squeeze(0)\n",
    "        # print(hidden.squeeze(0).shape)\n",
    "\n",
    "        return output, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D599IGCwfRH8"
   },
   "outputs": [],
   "source": [
    "# class DecoderRNN(nn.Module):\n",
    "#     def __init__(self, hidden_size, emb_size, output_size, dropout,attention):\n",
    "#         super().__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.output_size = output_size\n",
    "#         self.emb_size = emb_size\n",
    "#         self.attention = attention\n",
    "\n",
    "        \n",
    "#         self.embedding = nn.Embedding(self.output_size, self.emb_size)\n",
    "#         self.out = nn.Linear((self.hidden_size* 2) + self.hidden_size + self.emb_size, output_size)\n",
    "#         self.rnn = nn.GRU((self.hidden_size * 2) + self.emb_size, self.hidden_size)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, inputs, hidden,encoder_outputs, mask, output_len,inputs_len, teacher_forcing=False):\n",
    "#         # Input shape: [batch, output_len]\n",
    "#         # Hidden shape: [seq_len=1, batch_size, hidden_dim] (the last hidden state of the encoder)\n",
    "        \n",
    "#         output_len = output_len[0]\n",
    "\n",
    "#         if teacher_forcing:\n",
    "#             dec_input = inputs\n",
    "#             embed = self.dropout(self.embedding(dec_input))   # shape [batch, output_len, hidden_dim]\n",
    "#             out, hidden = self.rnn(embed,hidden)\n",
    "#             out = self.out(out)  # linear layer, out has now shape [batch, output_len, output_size]\n",
    "#             output = F.log_softmax(out, -1)\n",
    "#         else:\n",
    "#             # Take the EOS character only, for the whole batch, and unsqueeze so shape is [batch, 1]\n",
    "#             # This is the first input, then we will use as input the GRU output at the previous time step\n",
    "#             dec_input = inputs[:, 0].unsqueeze(1)\n",
    "\n",
    "#             output = []\n",
    "#             for i in range(output_len):\n",
    "#                 embedded = self.dropout(self.embedding(dec_input))\n",
    "#                 embedded = embedded.permute(1, 0, 2)\n",
    "#                 print(hidden.shape,encoder_outputs.shape,mask.shape)\n",
    "#                 #encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "#                 a = self.attention(hidden, encoder_outputs, mask)\n",
    "#                 a = a.unsqueeze(1)\n",
    "#                 encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "#                 #print(a.shape, encoder_outputs.shape)\n",
    "#                 weighted = torch.bmm(a, encoder_outputs)\n",
    "#                 weighted = weighted.permute(1, 0, 2)\n",
    "#                 #print(embedded.shape,weighted.shape)\n",
    "#                 rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "#                 #print(weighted.shape,embedded.shape,rnn_input.shape,hidden.shape)\n",
    "#                 out, hidden= self.rnn(rnn_input,hidden.unsqueeze(0))\n",
    "#                 #out, hidden = self.rnn(self.embedding(dec_input), hidden)\n",
    "#                 assert (out == hidden).all()\n",
    "#                 embedded = embedded.squeeze(0)\n",
    "#                 out = out.squeeze(0)\n",
    "#                 weighted = weighted.squeeze(0)\n",
    "\n",
    "#                 output = self.out(torch.cat((out, weighted, embedded), dim = 1))\n",
    "#                 #print(output.shape,hidden.squeeze(0).shape)\n",
    "\n",
    "#             #output = torch.stack(output).permute(1, 0, 2)  # [batch_size x seq_len x output_size]\n",
    "#                 hidden = hidden.squeeze(0)\n",
    "#                 print(hidden.squeeze(0).shape)\n",
    "\n",
    "#         return output,hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "emvRrQ3ID0Bs"
   },
   "source": [
    "The learned representation from the *Encoder* gets propagated to the *Decoder* as the final hidden layer in the *Encoder* network is set as initialisation for the *Decoder*'s first hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQ2vLb3CfRIE"
   },
   "outputs": [],
   "source": [
    "def create_mask(src):\n",
    "    mask = src != 0  # .permute(1, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EyWCT1sLD0Bu"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def forward_pass(\n",
    "    encoder, decoder, x, t, t_in, x_len, criterion, teacher_forcing_ratio=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes a forward pass through the whole model.\n",
    "\n",
    "    :param encoder:\n",
    "    :param decoder:\n",
    "    :param x: input to the encoder, shape [batch, seq_in_len]\n",
    "    :param t: target output predictions for decoder, shape [batch, seq_t_len]\n",
    "    :param criterion: loss function\n",
    "    :param max_t_len: maximum target length\n",
    "\n",
    "    :return: output (after log-softmax), loss, accuracy (per-symbol)\n",
    "    \"\"\"\n",
    "    batch_size = x.shape[0]\n",
    "    # print(batch_size)\n",
    "    trg_len = t_in.shape[1]\n",
    "    # print(t_in.shape)\n",
    "    trg_vocab_size = NUM_OUTPUTS\n",
    "\n",
    "    # tensor to store decoder outputs\n",
    "    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size)\n",
    "\n",
    "    # Run encoder and get last hidden state (and output)\n",
    "\n",
    "    enc_h = encoder.init_hidden(batch_size)\n",
    "    enc_out, enc_h = encoder(x, enc_h, x_len)\n",
    "    # print(enc_h.shape,enc_out.shape)\n",
    "\n",
    "    # print(mask.shape)\n",
    "\n",
    "    # first input to the decoder is the <sos> tokens\n",
    "    inputs = t_in[:, 0]\n",
    "    # print(inputs)\n",
    "    dec_h = enc_h\n",
    "    mask = create_mask(x)\n",
    "\n",
    "    for i in range(1, trg_len + 1):\n",
    "\n",
    "        # insert input token embedding, previous hidden state, all encoder hidden states\n",
    "        #  and mask\n",
    "        # receive output tensor (predictions) and new hidden state\n",
    "        output, dec_h, _ = decoder(inputs, dec_h, enc_out, mask)\n",
    "\n",
    "        # place predictions in a tensor holding predictions for each token\n",
    "        outputs[i - 1] = output\n",
    "\n",
    "        # decide if we are going to use teacher forcing or not\n",
    "        teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "        # get the highest predicted token from our predictions\n",
    "        top1 = output.argmax(1)\n",
    "\n",
    "        # if teacher forcing, use actual next token as next input\n",
    "        # if not, use predicted token\n",
    "        if i < trg_len:\n",
    "            inputs = t_in[:, i] if teacher_force else top1\n",
    "\n",
    "    out = outputs.permute(1, 2, 0)\n",
    "    # Shape: [batch_size x num_classes x out_sequence_len], with second dim containing log probabilities\n",
    "    # print(out.shape,t.shape)\n",
    "    loss = criterion(out, t)\n",
    "    pred = get_pred(log_probs=out)\n",
    "    accuracy = (pred == t).type(torch.FloatTensor).mean()\n",
    "\n",
    "    return out, loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HODw6ZgyfRIP"
   },
   "outputs": [],
   "source": [
    "# def forward_pass(encoder, decoder, x, t, t_in,x_len,criterion,max_t_len,teacher_forcing):\n",
    "#     \"\"\"\n",
    "#     Executes a forward pass through the whole model.\n",
    "\n",
    "#     :param encoder:\n",
    "#     :param decoder:\n",
    "#     :param x: input to the encoder, shape [batch, seq_in_len]\n",
    "#     :param t: target output predictions for decoder, shape [batch, seq_t_len]\n",
    "#     :param criterion: loss function\n",
    "#     :param max_t_len: maximum target length\n",
    "\n",
    "#     :return: output (after log-softmax), loss, accuracy (per-symbol)\n",
    "#     \"\"\"\n",
    "\n",
    "    \n",
    "#     # Run encoder and get last hidden state (and output)\n",
    "#     #print(x)\n",
    "#     #print(len(x))\n",
    "#     batch_size = len(x)\n",
    "#     enc_h = encoder.init_hidden(batch_size)\n",
    "#     enc_out, enc_h = encoder(x, enc_h,x_len)\n",
    "#     #print(enc_h.shape,enc_out.shape)\n",
    "    \n",
    "#     def create_mask(src):\n",
    "#         mask = (src != 0)#.permute(1, 0)\n",
    "#         return mask\n",
    "    \n",
    "    \n",
    "#     mask = create_mask(x)\n",
    "#     #print(mask.shape)\n",
    "\n",
    "#     dec_h = enc_h  # Init hidden state of decoder as hidden state of encoder\n",
    "#     #print(dec_h.shape)\n",
    "#     dec_input = t_in\n",
    "#     out,dec_h,_ = decoder(dec_input, dec_h,enc_out,mask, max_t_len, teacher_forcing)\n",
    "#     #print(dec_h.shape)\n",
    "#     out = out.permute(0, 2, 1)\n",
    "#     # Shape: [batch_size x num_classes x out_sequence_len], with second dim containing log probabilities\n",
    "\n",
    "#     loss = criterion(out, t)\n",
    "#     pred = get_pred(log_probs=out)\n",
    "#     accuracy = (pred == t).type(torch.FloatTensor).mean()\n",
    "#     return out, loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4G_hLF0GD0By"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    inputs,\n",
    "    targets,\n",
    "    targets_in,\n",
    "    criterion,\n",
    "    enc_optimizer,\n",
    "    dec_optimizer,\n",
    "    epoch,\n",
    "    inputs_len,\n",
    "):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (x, t, t_in, x_len) in enumerate(\n",
    "        zip(inputs, targets, targets_in, inputs_len)\n",
    "    ):\n",
    "        # print(x.shape)\n",
    "        x = torch.LongTensor(x).to(device)\n",
    "        t = torch.LongTensor(t).to(device)\n",
    "        t_in = torch.LongTensor(t_in).to(device)\n",
    "        x_len = torch.LongTensor(x_len).to(device)\n",
    "\n",
    "        enc_optimizer.zero_grad()\n",
    "        dec_optimizer.zero_grad()\n",
    "\n",
    "        # print(batch_idx)\n",
    "        #         inputs = inputs.to(device)\n",
    "        #         targets = targets.long()\n",
    "        #         targets_in = targets_in.long()\n",
    "        out, loss, accuracy = forward_pass(\n",
    "            encoder,\n",
    "            decoder,\n",
    "            x,\n",
    "            t,\n",
    "            t_in,\n",
    "            x_len,\n",
    "            criterion,\n",
    "            teacher_forcing_ratio=TEACHER_FORCING,\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        enc_optimizer.step()\n",
    "        dec_optimizer.step()\n",
    "        if batch_idx % 200 == 0:\n",
    "            print(\n",
    "                \"Epoch {} [{}/{} ({:.0f}%)]\\tTraining loss: {:.4f} \\tTraining accuracy: {:.1f}%\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(x),\n",
    "                    TRAINING_SIZE * NUM_OF_BATCHES,\n",
    "                    100.0\n",
    "                    * batch_idx\n",
    "                    * len(x)\n",
    "                    / (TRAINING_SIZE * NUM_OF_BATCHES),\n",
    "                    loss.item(),\n",
    "                    100.0 * accuracy.item(),\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QWR1jGl5D0B3"
   },
   "outputs": [],
   "source": [
    "def test(encoder, decoder, inputs, targets, targets_in, inputs_len, criterion):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        #         inputs = inputs.to(device)\n",
    "        #         print(targets)\n",
    "        #         targets = targets.long()\n",
    "        #         targets_in = targets_in.long()\n",
    "        inputs = inputs.view(inputs.shape[1], inputs.shape[2])\n",
    "        targets = targets.view(targets.shape[1], targets.shape[2])\n",
    "        targets_in = targets_in.view(targets_in.shape[1], targets_in.shape[2])\n",
    "        inputs_len = torch.LongTensor(inputs_len[0]).to(device)\n",
    "        # print(inputs_len)\n",
    "\n",
    "        out, loss, accuracy = forward_pass(\n",
    "            encoder,\n",
    "            decoder,\n",
    "            inputs,\n",
    "            targets,\n",
    "            targets_in,\n",
    "            inputs_len,\n",
    "            criterion,\n",
    "            teacher_forcing_ratio=TEACHER_FORCING,\n",
    "        )\n",
    "        # print(out.shape,targets_in.shape)\n",
    "    return out, loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fr_FfcipD0B-"
   },
   "outputs": [],
   "source": [
    "def numbers_to_text(seq):\n",
    "    return \"\".join([str(to_np(i)) if to_np(i) != 10 else \"#\" for i in seq])\n",
    "\n",
    "\n",
    "def to_np(x):\n",
    "    return x.cpu().numpy()\n",
    "\n",
    "\n",
    "def get_pred(log_probs):\n",
    "    \"\"\"\n",
    "    Get class prediction (digit prediction) from the net's output (the log_probs)\n",
    "    :param log_probs: Tensor of shape [batch_size x n_classes x sequence_len]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return torch.argmax(log_probs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 231769,
     "status": "ok",
     "timestamp": 1575204780896,
     "user": {
      "displayName": "Alexandros Dorodoulis",
      "photoUrl": "",
      "userId": "07816985308410186091"
     },
     "user_tz": -60
    },
    "id": "k1lh5zCfD0CB",
    "outputId": "db7527ce-7805-4706-8ee3-e4af3581e47a"
   },
   "outputs": [],
   "source": [
    "attn = Attention(NUM_UNITS_ENC)\n",
    "encoder = EncoderRNN(NUM_INPUTS, HIDDEN_DIM, NUM_UNITS_ENC, DROPOUT).to(device)\n",
    "decoder = DecoderRNN(NUM_UNITS_DEC, HIDDEN_DIM, NUM_OUTPUTS, DROPOUT, attn).to(\n",
    "    device\n",
    ")\n",
    "enc_optimizer = optim.RMSprop(encoder.parameters(), lr=LEARNING_RATE)\n",
    "dec_optimizer = optim.RMSprop(decoder.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\", ignore_index=0)\n",
    "# criterion = nn.NLLLoss(weight=None, reduction=\"mean\", ignore_index=0)\n",
    "\n",
    "# Get training set\n",
    "(\n",
    "    inputs,\n",
    "    _,\n",
    "    targets_in,\n",
    "    targets,\n",
    "    targets_seqlen,\n",
    "    _,\n",
    "    text,\n",
    "    _,\n",
    "    text_targ,\n",
    "    inputs_len,\n",
    ") = generate(\n",
    "    TRAINING_SIZE, NUM_OF_BATCHES, min_len=MIN_SEQ_LEN, max_len=MAX_SEQ_LEN\n",
    ")\n",
    "max_target_len = max(targets_seqlen)\n",
    "# inputs = torch.tensor(inputs)\n",
    "# inputs = torch.LongTensor(inputs)\n",
    "# targets = torch.LongTensor(targets)\n",
    "# targets_in = torch.LongTensor(targets_in)\n",
    "unique_text_targets = set([i for x in text_targ for i in x])\n",
    "\n",
    "# Get validation set\n",
    "(\n",
    "    val_inputs,\n",
    "    _,\n",
    "    val_targets_in,\n",
    "    val_targets,\n",
    "    val_targets_seqlen,\n",
    "    _,\n",
    "    val_text_in,\n",
    "    _,\n",
    "    val_text_targ,\n",
    "    val_inputs_len,\n",
    ") = generate(\n",
    "    1,\n",
    "    TEST_SIZE,\n",
    "    min_len=MIN_SEQ_LEN,\n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    invalid_set=unique_text_targets,\n",
    ")\n",
    "# val_inputs = torch.tensor(val_inputs)\n",
    "val_inputs = torch.LongTensor(val_inputs).to(device)\n",
    "val_targets = torch.LongTensor(val_targets).to(device)\n",
    "val_targets_in = torch.LongTensor(val_targets_in).to(device)\n",
    "val_inputs_len = torch.LongTensor(val_inputs_len).to(device)\n",
    "max_val_target_len = max(val_targets_seqlen)\n",
    "\n",
    "\n",
    "# Quick and dirty - just loop over training set without reshuffling\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(\n",
    "        encoder,\n",
    "        decoder,\n",
    "        inputs,\n",
    "        targets,\n",
    "        targets_in,\n",
    "        criterion,\n",
    "        enc_optimizer,\n",
    "        dec_optimizer,\n",
    "        epoch,\n",
    "        inputs_len,\n",
    "    )\n",
    "    _, loss, accuracy = test(\n",
    "        encoder,\n",
    "        decoder,\n",
    "        val_inputs,\n",
    "        val_targets,\n",
    "        val_targets_in,\n",
    "        val_inputs_len,\n",
    "        criterion,\n",
    "    )\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f} \\tAccuracy: {:.3f}%\\n\".format(\n",
    "            loss, accuracy.item() * 100.0\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Show examples\n",
    "    print(\"Examples: prediction | input\")\n",
    "    out, _, _ = test(\n",
    "        encoder,\n",
    "        decoder,\n",
    "        val_inputs[:10],\n",
    "        val_targets[:10],\n",
    "        val_targets_in[:10],\n",
    "        val_inputs_len[:10],\n",
    "        criterion,\n",
    "    )\n",
    "    pred = get_pred(out)\n",
    "    pred_text = [numbers_to_text(sample) for sample in pred]\n",
    "\n",
    "    # print(len(pred_text)) range used to be 10\n",
    "    for i in range(9):\n",
    "        print(pred_text[i], \"\\t\", val_text_in[0][i])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "5.4-EXE-seq2seq-digits_Amalia.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
