{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J5tgIb-fsjZd"
   },
   "source": [
    "This code was heavily influenced by the following:\n",
    "5.4-EXE-seq2seq-digits : Jupyter notebook that was provided in week 5 \n",
    "https://github.com/bentrevett/pytorch-seq2seq\n",
    "https://github.com/bastings/annotated_encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 6213,
     "status": "ok",
     "timestamp": 1575405986862,
     "user": {
      "displayName": "Alexandros Dorodoulis",
      "photoUrl": "",
      "userId": "07816985308410186091"
     },
     "user_tz": -60
    },
    "id": "nfC2KkiXu4nd",
    "outputId": "b385b496-06e5-4bfe-c3e9-7979c9ae179a"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade torch numpy matplotlib sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "LpYJqpvTsT5x"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "uModMG0rsT6A"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import sacrebleu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.core.debugger import set_trace\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6186,
     "status": "ok",
     "timestamp": 1575405986866,
     "user": {
      "displayName": "Alexandros Dorodoulis",
      "photoUrl": "",
      "userId": "07816985308410186091"
     },
     "user_tz": -60
    },
    "id": "QCdRmVowsT6J",
    "outputId": "981b024b-d86e-4fc0-cf99-b7c246eaf479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: False\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda:0\")  # or set to 'cpu'\n",
    "print(\"CUDA:\", USE_CUDA)\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "GiVDUOGzsT6P"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "D8SpjFDZsT6T"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many\n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.trg_embed = trg_embed\n",
    "        self.generator = generator\n",
    "    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n",
    "        \"\"\"Take in and process masked src and target sequences.\"\"\"\n",
    "        encoder_hidden, encoder_final = self.encode(src, src_mask, src_lengths)\n",
    "        return self.decode(\n",
    "            encoder_hidden, encoder_final, src_mask, trg, trg_mask\n",
    "        )\n",
    "    def encode(self, src, src_mask, src_lengths):\n",
    "        return self.encoder(self.src_embed(src), src_mask, src_lengths)\n",
    "    def decode(\n",
    "        self,\n",
    "        encoder_hidden,\n",
    "        encoder_final,\n",
    "        src_mask,\n",
    "        trg,\n",
    "        trg_mask,\n",
    "        decoder_hidden=None,\n",
    "        max_len=None,\n",
    "    ):\n",
    "        return self.decoder(\n",
    "            self.trg_embed(trg),\n",
    "            encoder_hidden,\n",
    "            encoder_final,\n",
    "            src_mask,\n",
    "            trg_mask,\n",
    "            hidden=decoder_hidden,\n",
    "            max_len=max_len,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EeOMNogDsT6a"
   },
   "source": [
    "\n",
    "Projecting the pre-output layer ($x$ in the `forward` function below) to obtain the output layer, so that the final dimension is the target vocabulary size.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Dcoo05KmsT6b"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ivQgiTD9sT64"
   },
   "source": [
    "\n",
    "## Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bSuDKvuPsT66"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encodes a sequence of word embeddings\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "    def forward(self, x, mask, lengths):\n",
    "        \"\"\"\n",
    "        Applies a bidirectional GRU to sequence of embeddings x.\n",
    "        The input mini-batch x needs to be sorted by length.\n",
    "        x should have dimensions [batch, time, dim].\n",
    "        \"\"\"\n",
    "        packed = pack_padded_sequence(\n",
    "            x, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        output, final = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(\n",
    "            output, batch_first=True, padding_value=0\n",
    "        )\n",
    "\n",
    "        # we need to manually concatenate the final states for both directions\n",
    "        fwd_final = final[0 : final.size(0) : 2]\n",
    "        bwd_final = final[1 : final.size(0) : 2]\n",
    "        final = torch.cat(\n",
    "            [fwd_final, bwd_final], dim=2\n",
    "        )  # [num_layers, batch, 2*dim]\n",
    "        return output, final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "38dkONDqsT6-"
   },
   "source": [
    "\n",
    "### Decoder<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bhY-nNIesT7A"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_size,\n",
    "        hidden_size,\n",
    "        num_layers=1,\n",
    "        dropout=0.5,\n",
    "        bridge=True,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.rnn = nn.GRU(\n",
    "            emb_size ,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        # to initialize from the final encoder state\n",
    "        self.bridge = (\n",
    "            nn.Linear(2 * hidden_size, hidden_size, bias=True)\n",
    "            if bridge\n",
    "            else None\n",
    "        )\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "    def forward_step(\n",
    "        self, prev_embed, encoder_hidden, src_mask, hidden\n",
    "    ):\n",
    "        \"\"\"Perform a single decoder step (1 word)\"\"\"\n",
    "\n",
    "\n",
    "        # update rnn hidden state\n",
    "        #print(prev_embed.shape,hidden.shape)\n",
    "        output, hidden = self.rnn(prev_embed, hidden)\n",
    "        pre_output = self.dropout_layer(output)\n",
    "        return output, hidden, pre_output\n",
    "    def forward(\n",
    "        self,\n",
    "        trg_embed,\n",
    "        encoder_hidden,\n",
    "        encoder_final,\n",
    "        src_mask,\n",
    "        trg_mask,\n",
    "        hidden=None,\n",
    "        max_len=None,\n",
    "    ):\n",
    "        \"\"\"Unroll the decoder one step at a time.\"\"\"\n",
    "\n",
    "        # the maximum number of steps to unroll the RNN\n",
    "        if max_len is None:\n",
    "            max_len = trg_mask.size(-1)\n",
    "\n",
    "        # initialize decoder hidden state\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(encoder_final)\n",
    "\n",
    "\n",
    "        # here we store all intermediate hidden states and pre-output vectors\n",
    "        decoder_states = []\n",
    "        pre_output_vectors = []\n",
    "\n",
    "        # unroll the decoder RNN for max_len steps\n",
    "        for i in range(max_len):\n",
    "            prev_embed = trg_embed[:, i].unsqueeze(1)\n",
    "            output, hidden, pre_output = self.forward_step(\n",
    "                prev_embed, encoder_hidden, src_mask, hidden\n",
    "            )\n",
    "            decoder_states.append(output)\n",
    "            pre_output_vectors.append(pre_output)\n",
    "        decoder_states = torch.cat(decoder_states, dim=1)\n",
    "        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n",
    "        return decoder_states, hidden, pre_output_vectors  # [B, N, D]\n",
    "    def init_hidden(self, encoder_final):\n",
    "        \"\"\"Returns the initial decoder state,\n",
    "        conditioned on the final encoder state.\"\"\"\n",
    "        if encoder_final is None:\n",
    "            return None  # start with zeros\n",
    "        return torch.tanh(self.bridge(encoder_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2vLDCPXKsT7K"
   },
   "source": [
    "\n",
    "## Full Model<br>\n",
    "Here we define a function from hyperparameters to a full model.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fdl87K6QsT7L"
   },
   "outputs": [],
   "source": [
    "def make_model(\n",
    "    src_vocab,\n",
    "    tgt_vocab,\n",
    "    emb_size=64,\n",
    "    hidden_size=128,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    "):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(emb_size, hidden_size, num_layers=num_layers, dropout=dropout),\n",
    "        Decoder(\n",
    "            emb_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "        ),\n",
    "        nn.Embedding(src_vocab, emb_size, padding_idx=0),\n",
    "        nn.Embedding(tgt_vocab, emb_size, padding_idx=0),\n",
    "        Generator(hidden_size, tgt_vocab),\n",
    "    )\n",
    "    return model.cuda() if USE_CUDA else model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "liKUcy4SsT7P"
   },
   "source": [
    "\n",
    "# Training<br>\n",
    "\n",
    "## Batches and Masking<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "E_SNhxuTsT7Q"
   },
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training.\n",
    "    Input is a batch from a torch text iterator.\n",
    "    \"\"\"\n",
    "    def __init__(self, src, trg, pad_index=0):\n",
    "        src, src_lengths = src\n",
    "        self.src = src\n",
    "        self.src_lengths = src_lengths\n",
    "        self.src_mask = (src != pad_index).unsqueeze(-2)\n",
    "        self.nseqs = src.size(0)\n",
    "        self.trg = None\n",
    "        self.trg_y = None\n",
    "        self.trg_mask = None\n",
    "        self.trg_lengths = None\n",
    "        self.ntokens = None\n",
    "        if trg is not None:\n",
    "            trg, trg_lengths = trg\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_lengths = trg_lengths\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = self.trg_y != pad_index\n",
    "            self.ntokens = (self.trg_y != pad_index).data.sum().item()\n",
    "        if USE_CUDA:\n",
    "            self.src = self.src.cuda()\n",
    "            self.src_mask = self.src_mask.cuda()\n",
    "            if trg is not None:\n",
    "                self.trg = self.trg.cuda()\n",
    "                self.trg_y = self.trg_y.cuda()\n",
    "                self.trg_mask = self.trg_mask.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9rvXNXofsT7U"
   },
   "source": [
    "\n",
    "## Training Loop<br>\n",
    "The code below trains the model for 1 epoch (=1 pass through the training data).<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Dg6BNbqHsT7W"
   },
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute, print_every=50):\n",
    "    \"\"\"Standard Training and Logging Function\"\"\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    print_tokens = 0\n",
    "    for i, batch in enumerate(data_iter, 1):\n",
    "        out, _, pre_output = model.forward(\n",
    "            batch.src,\n",
    "            batch.trg,\n",
    "            batch.src_mask,\n",
    "            batch.trg_mask,\n",
    "            batch.src_lengths,\n",
    "            batch.trg_lengths,\n",
    "        )\n",
    "        loss = loss_compute(pre_output, batch.trg_y, batch.nseqs)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        print_tokens += batch.ntokens\n",
    "        if model.training and i % print_every == 0:\n",
    "            elapsed = time.time() - start\n",
    "            print(\n",
    "                \"Epoch Step: %d Loss: %f Tokens per Sec: %f\"\n",
    "                % (i, loss / batch.nseqs, print_tokens / elapsed)\n",
    "            )\n",
    "            start = time.time()\n",
    "            print_tokens = 0\n",
    "    return math.exp(total_loss / float(total_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bh2mPUjrsT7c"
   },
   "source": [
    "\n",
    "## Synthetic Data<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "g177F3EbsT7d"
   },
   "outputs": [],
   "source": [
    "target_to_text = {\n",
    "    \"0\": \"0\",\n",
    "    \"1\": \"1\",\n",
    "    \"2\": \"two\",\n",
    "    \"3\": \"three\",\n",
    "    \"4\": \"four\",\n",
    "    \"5\": \"five\",\n",
    "    \"6\": \"six\",\n",
    "    \"7\": \"seven\",\n",
    "    \"8\": \"eight\",\n",
    "    \"9\": \"nine\",\n",
    "}\n",
    "input_characters = \" \".join(target_to_text.values())\n",
    "valid_characters = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\",] + list(\n",
    "    set(input_characters)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "aaeoC9-8sT7i"
   },
   "outputs": [],
   "source": [
    "src_vocab_len = len(valid_characters)\n",
    "trg_vocab_len = len(target_to_text.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "o2rOJtb6sT7o"
   },
   "outputs": [],
   "source": [
    "def data_gen(\n",
    "    num_words=9,\n",
    "    batch_size=16,\n",
    "    num_batches=100,\n",
    "    min_length=3,\n",
    "    max_length=8,\n",
    "    pad_index=0,\n",
    "    eos_index=1,\n",
    "    sos_index=1,\n",
    "):\n",
    "    \"\"\"Generate random data for a src-tgt copy task.\"\"\"\n",
    "    for i in range(num_batches):\n",
    "        data = [\n",
    "            np.random.randint(\n",
    "                2,\n",
    "                num_words,\n",
    "                size=(np.random.randint(min_length, max_length + 1)),\n",
    "            )\n",
    "            for i in range(batch_size)\n",
    "        ]\n",
    "        for arr in data:\n",
    "            arr[-1] = eos_index\n",
    "            arr[0] = sos_index\n",
    "        trg_max_length = max([len(i) for i in data])\n",
    "        tmp = np.zeros((batch_size, trg_max_length), dtype=\"int64\")\n",
    "        trg_lengths = []\n",
    "        for i, arr in enumerate(data):\n",
    "            cur_len = len(arr)\n",
    "            trg_lengths.append(cur_len)\n",
    "            tmp[i, :cur_len] = arr\n",
    "        data = tmp\n",
    "        src = [\n",
    "            [\n",
    "                target_to_text[str(x)]\n",
    "                for x in i\n",
    "                if x not in (pad_index, eos_index, sos_index)\n",
    "            ]\n",
    "            for i in data\n",
    "        ]\n",
    "        src = [[valid_characters.index(el) for el in \" \".join(y)] for y in src]\n",
    "        src_max_len = max([len(i) for i in src])\n",
    "        src_lengths = []\n",
    "        tmp = np.zeros((batch_size, src_max_len), dtype=\"int64\")\n",
    "        for i, arr in enumerate(src):\n",
    "            cur_len = len(arr)\n",
    "            src_lengths.append(cur_len)\n",
    "            tmp[i, :cur_len] = arr\n",
    "        src = torch.from_numpy(tmp)\n",
    "        data = torch.from_numpy(data)\n",
    "        data = data.cuda() if USE_CUDA else data\n",
    "        trg = data\n",
    "        yield Batch(\n",
    "            (torch.LongTensor(src), src_lengths),\n",
    "            (torch.LongTensor(trg), trg_lengths),\n",
    "            pad_index=pad_index,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aT3Pq2eZsT7u"
   },
   "source": [
    "\n",
    "## Loss Computation\n",
    "  \n",
    "A simple loss compute and train function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3zKPw-mbsT7v"
   },
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(\n",
    "            x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n",
    "        )\n",
    "        loss = loss / norm\n",
    "        if self.opt is not None:\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "        return loss.data.item() * norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p2QU5Tc_sT71"
   },
   "source": [
    "\n",
    "### Printing examples<br>\n",
    "\n",
    "We use greedy decoding for simplicity; that is, at each time step, starting at the first token, we choose the one with that maximum probability, and we never revisit that choice.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "FQbJj6wZsT72"
   },
   "outputs": [],
   "source": [
    "def greedy_decode(\n",
    "    model, src, src_mask, src_lengths, max_len=10, sos_index=1, eos_index=1\n",
    "):\n",
    "    \"\"\"Greedily decode a sentence.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden, encoder_final = model.encode(src, src_mask, src_lengths)\n",
    "        prev_y = torch.ones(1, 1).fill_(sos_index).type_as(src)\n",
    "        trg_mask = torch.ones_like(prev_y)\n",
    "    output = []\n",
    "    hidden = None\n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            out, hidden, pre_output = model.decode(\n",
    "                encoder_hidden,\n",
    "                encoder_final,\n",
    "                src_mask,\n",
    "                prev_y,\n",
    "                trg_mask,\n",
    "                hidden,\n",
    "            )\n",
    "\n",
    "            # we predict from the pre-output layer, which is\n",
    "            # a combination of Decoder state, prev emb, and context\n",
    "            prob = model.generator(pre_output[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data.item()\n",
    "        output.append(next_word)\n",
    "        prev_y = torch.ones(1, 1).type_as(src).fill_(next_word)\n",
    "    output = np.array(output)\n",
    "\n",
    "    # cut off everything starting from </s>\n",
    "    # (only when eos_index provided)\n",
    "    if eos_index is not None:\n",
    "        first_eos = np.where(output == eos_index)[0]\n",
    "        if len(first_eos) > 0:\n",
    "            output = output[: first_eos[0]]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "YCJ7xtjGsT75"
   },
   "outputs": [],
   "source": [
    "def lookup_words(x, vocab=None):\n",
    "    if vocab is not None:\n",
    "        x = [vocab.itos[i] for i in x]\n",
    "    return [str(t) for t in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wkl5-IecsT7-"
   },
   "outputs": [],
   "source": [
    "def turn_num_to_text(nums):\n",
    "    return [valid_characters[num] for num in nums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "YfOttqobsT8B"
   },
   "outputs": [],
   "source": [
    "def print_examples(\n",
    "    example_iter,\n",
    "    model,\n",
    "    n=2,\n",
    "    max_len=10,\n",
    "    sos_index=1,\n",
    "    src_eos_index=None,\n",
    "    trg_eos_index=1,\n",
    "    src_vocab=None,\n",
    "    trg_vocab=None,\n",
    "):\n",
    "    \"\"\"Prints N examples. Assumes batch size of 1.\"\"\"\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    print()\n",
    "    if src_vocab is not None and trg_vocab is not None:\n",
    "        src_eos_index = src_vocab.stoi[EOS_TOKEN]\n",
    "        trg_sos_index = trg_vocab.stoi[SOS_TOKEN]\n",
    "        trg_eos_index = trg_vocab.stoi[EOS_TOKEN]\n",
    "    else:\n",
    "        src_eos_index = None\n",
    "        trg_sos_index = 1\n",
    "        trg_eos_index = 1\n",
    "    for i, batch in enumerate(example_iter):\n",
    "        src = batch.src.cpu().numpy()[0, :]\n",
    "        trg = batch.trg_y.cpu().numpy()[0, :]\n",
    "\n",
    "        # remove </s> (if it is there)\n",
    "        src = src[:-1] if src[-1] == src_eos_index else src\n",
    "        trg = trg[:-1] if trg[-1] == trg_eos_index else trg\n",
    "        result = greedy_decode(\n",
    "            model,\n",
    "            batch.src,\n",
    "            batch.src_mask,\n",
    "            batch.src_lengths,\n",
    "            sos_index=trg_sos_index,\n",
    "            eos_index=trg_eos_index,\n",
    "        )\n",
    "        match = 0\n",
    "        print(\"Example #%d\" % (i + 1))\n",
    "        print(\"Src : \", \"\".join(turn_num_to_text(src)))\n",
    "        print(\"Trg : \", \" \".join(lookup_words(trg, vocab=trg_vocab)))\n",
    "        print(\"Pred: \", \" \".join(lookup_words(result, vocab=trg_vocab)))\n",
    "        print()\n",
    "        count += 1\n",
    "        print()\n",
    "        if count == n:\n",
    "            break\n",
    "    return src,trg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjJDIiiVsT8F"
   },
   "source": [
    "\n",
    "## Training the \"translating\" task\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Mf-aGh6qsT8G"
   },
   "outputs": [],
   "source": [
    "def train_trans_task():\n",
    "    num_words = 10\n",
    "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=0)\n",
    "    model = make_model(\n",
    "        src_vocab_len, trg_vocab_len, emb_size=32, hidden_size=64\n",
    "    )\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "    min_length = 4\n",
    "    max_length = 150\n",
    "    batch_size = 32\n",
    "    num_batches = 150\n",
    "    eval_data = list(\n",
    "        data_gen(\n",
    "            num_words=num_words,\n",
    "            batch_size=1,\n",
    "            num_batches=num_batches,\n",
    "            min_length=min_length,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "    )\n",
    "    dev_perplexities = []\n",
    "    if USE_CUDA:\n",
    "        model.cuda()\n",
    "    for epoch in range(10):\n",
    "        print(\"Epoch %d\" % epoch)\n",
    "        data = data_gen(\n",
    "            num_words=num_words,\n",
    "            batch_size=batch_size,\n",
    "            num_batches=num_batches,\n",
    "            min_length=min_length,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        # train\n",
    "        model.train()\n",
    "        run_epoch(\n",
    "            data, model, SimpleLossCompute(model.generator, criterion, optim)\n",
    "        )\n",
    "\n",
    "        # evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            perplexity = run_epoch(\n",
    "                eval_data,\n",
    "                model,\n",
    "                SimpleLossCompute(model.generator, criterion, None),\n",
    "            )\n",
    "            print(\"Evaluation perplexity: %f\" % perplexity)\n",
    "            dev_perplexities.append(perplexity)\n",
    "            #src_ex,trg_ex=print_examples(eval_data, model, n=2, max_len=max_length)\n",
    "    return dev_perplexities#,src_ex,trg_ex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "7prSJ-KBsT8L",
    "outputId": "92ead8e6-f0dc-40e9-fd56-9207da781016"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amalia\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch Step: 50 Loss: 164.726929 Tokens per Sec: 151.965372\n",
      "Epoch Step: 100 Loss: 203.113098 Tokens per Sec: 164.072132\n",
      "Epoch Step: 150 Loss: 162.904739 Tokens per Sec: 166.993928\n",
      "Evaluation perplexity: 8.300728\n",
      "Epoch 1\n",
      "Epoch Step: 50 Loss: 154.496826 Tokens per Sec: 157.744379\n",
      "Epoch Step: 100 Loss: 151.661118 Tokens per Sec: 161.732837\n",
      "Epoch Step: 150 Loss: 142.145813 Tokens per Sec: 166.023368\n",
      "Evaluation perplexity: 8.199388\n",
      "Epoch 2\n",
      "Epoch Step: 50 Loss: 152.990280 Tokens per Sec: 162.938273\n",
      "Epoch Step: 100 Loss: 139.293213 Tokens per Sec: 160.848474\n",
      "Epoch Step: 150 Loss: 154.508865 Tokens per Sec: 159.663959\n",
      "Evaluation perplexity: 8.129913\n",
      "Epoch 3\n",
      "Epoch Step: 50 Loss: 154.930649 Tokens per Sec: 165.481599\n",
      "Epoch Step: 100 Loss: 179.833450 Tokens per Sec: 158.378223\n",
      "Epoch Step: 150 Loss: 168.300095 Tokens per Sec: 152.126946\n",
      "Evaluation perplexity: 8.082966\n",
      "Epoch 4\n",
      "Epoch Step: 50 Loss: 148.695267 Tokens per Sec: 141.859177\n",
      "Epoch Step: 100 Loss: 149.059250 Tokens per Sec: 146.748745\n",
      "Epoch Step: 150 Loss: 143.294891 Tokens per Sec: 163.901446\n",
      "Evaluation perplexity: 8.029839\n",
      "Epoch 5\n",
      "Epoch Step: 50 Loss: 142.692230 Tokens per Sec: 169.508801\n",
      "Epoch Step: 100 Loss: 133.944901 Tokens per Sec: 167.421969\n",
      "Epoch Step: 150 Loss: 157.110092 Tokens per Sec: 159.758733\n",
      "Evaluation perplexity: 7.954526\n",
      "Epoch 6\n",
      "Epoch Step: 50 Loss: 178.242126 Tokens per Sec: 166.699045\n",
      "Epoch Step: 100 Loss: 146.414169 Tokens per Sec: 172.628656\n"
     ]
    }
   ],
   "source": [
    "dev_perplexities = train_trans_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "r3mTnjJesT8P"
   },
   "outputs": [],
   "source": [
    "def plot_perplexity(perplexities):\n",
    "    \"\"\"plot perplexities\"\"\"\n",
    "    plt.title(\"Perplexity per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.plot(perplexities)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "A5hCa-sJsT8T"
   },
   "outputs": [],
   "source": [
    "plot_perplexity(dev_perplexities)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "fMXg83uCsT7E",
    "2vLDCPXKsT7K"
   ],
   "name": "annotated_encoder_decoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
